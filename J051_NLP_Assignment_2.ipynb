{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIy3KV16w3VL"
      },
      "source": [
        "# NLP ASSIGNMENT 2\n",
        "\n",
        "## Yash Rohilla\n",
        "\n",
        "## J051"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QLdKDiRHxAsf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem.porter import *\n",
        "import re\n",
        "from gensim.utils import simple_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyRGLpNExAu8",
        "outputId": "baee6acd-d794-4f61-8000-9114cc587a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RmWvce4aybZU"
      },
      "outputs": [],
      "source": [
        "np.random.seed(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "eurz4FylxAxn",
        "outputId": "6a8bec82-b410-47eb-8fe9-fa6a37815717"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-daf42c58-b4ee-4526-ac90-43cac448ff0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-daf42c58-b4ee-4526-ac90-43cac448ff0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-daf42c58-b4ee-4526-ac90-43cac448ff0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-daf42c58-b4ee-4526-ac90-43cac448ff0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/papers.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kiF27XXxA0o",
        "outputId": "97941b40-68a6-4ce7-bcb7-8aa5c510236c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id               0\n",
              "year             0\n",
              "title            0\n",
              "event_type    4819\n",
              "pdf_name         0\n",
              "abstract         0\n",
              "paper_text       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEwCvG_zyfhR",
        "outputId": "752eef3e-45fc-4a45-a748-948134ab4562"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7241"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZXJEC4d1xA39",
        "outputId": "55d785a3-d057-4942-8034-eca08990105d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-adc14ba2-f942-4c16-bf39-32d18a6528ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adc14ba2-f942-4c16-bf39-32d18a6528ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-adc14ba2-f942-4c16-bf39-32d18a6528ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-adc14ba2-f942-4c16-bf39-32d18a6528ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     id                                         paper_text\n",
              "0     1  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n",
              "1    10  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n",
              "2   100  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n",
              "3  1000  Bayesian Query Construction for Neural\\nNetwor...\n",
              "4  1001  Neural Network Ensembles, Cross\\nValidation, a..."
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df = df[['id', 'paper_text']]\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2imC8aucB2JC"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cg4gi0-2z1fP"
      },
      "outputs": [],
      "source": [
        "#stemming with lemmatization\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQyTByrz1vZY",
        "outputId": "8c430f5a-0cc6-4187-e275-7916ec4854f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7vrYDJhz1ib",
        "outputId": "2d4a6d7c-988e-48d6-93ef-e5335c3e7aad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [self, organ, associ, databas, applic, hisashi...\n",
              "1    [mean, field, theori, layer, visual, cortex, a...\n",
              "2    [store, covari, associ, long, term, potenti, d...\n",
              "3    [bayesian, queri, construct, neural, network, ...\n",
              "4    [neural, network, ensembl, cross, valid, activ...\n",
              "Name: paper_text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "processed_text = df['paper_text'].map(preprocess)\n",
        "processed_text[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JT1n3zgPz1lO"
      },
      "outputs": [],
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uy_DBdC4kqg",
        "outputId": "1285c71d-f0b3-4508-ec3b-11cf15e02918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 abolish\n",
            "1 abstract\n",
            "2 acceler\n",
            "3 accept\n",
            "4 accomplish\n",
            "5 accord\n",
            "6 achiev\n",
            "7 actual\n",
            "8 adap\n",
            "9 address\n",
            "10 adjac\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IRYxchcP4ks6"
      },
      "outputs": [],
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV2GdHub4kvB",
        "outputId": "41462743-d0b8-4fe1-9062-4dcf195b0a63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1),\n",
              " (4, 1),\n",
              " (14, 1),\n",
              " (16, 3),\n",
              " (18, 1),\n",
              " (24, 2),\n",
              " (27, 1),\n",
              " (32, 2),\n",
              " (35, 1),\n",
              " (36, 1),\n",
              " (51, 1),\n",
              " (53, 1),\n",
              " (76, 1),\n",
              " (77, 3),\n",
              " (93, 2),\n",
              " (99, 1),\n",
              " (101, 2),\n",
              " (104, 1),\n",
              " (109, 1),\n",
              " (120, 1),\n",
              " (122, 3),\n",
              " (124, 4),\n",
              " (129, 1),\n",
              " (133, 3),\n",
              " (134, 2),\n",
              " (136, 6),\n",
              " (146, 5),\n",
              " (153, 1),\n",
              " (169, 2),\n",
              " (171, 1),\n",
              " (175, 1),\n",
              " (189, 1),\n",
              " (193, 1),\n",
              " (198, 1),\n",
              " (205, 18),\n",
              " (211, 1),\n",
              " (212, 1),\n",
              " (213, 2),\n",
              " (241, 2),\n",
              " (244, 2),\n",
              " (245, 7),\n",
              " (252, 1),\n",
              " (255, 1),\n",
              " (261, 1),\n",
              " (263, 4),\n",
              " (265, 1),\n",
              " (266, 7),\n",
              " (269, 1),\n",
              " (271, 1),\n",
              " (278, 7),\n",
              " (285, 1),\n",
              " (291, 2),\n",
              " (297, 2),\n",
              " (302, 4),\n",
              " (305, 4),\n",
              " (307, 1),\n",
              " (308, 1),\n",
              " (310, 4),\n",
              " (311, 1),\n",
              " (317, 37),\n",
              " (318, 2),\n",
              " (320, 5),\n",
              " (323, 18),\n",
              " (338, 5),\n",
              " (348, 1),\n",
              " (364, 1),\n",
              " (365, 1),\n",
              " (366, 1),\n",
              " (368, 2),\n",
              " (379, 1),\n",
              " (388, 1),\n",
              " (392, 1),\n",
              " (393, 3),\n",
              " (400, 6),\n",
              " (402, 4),\n",
              " (420, 1),\n",
              " (440, 4),\n",
              " (450, 1),\n",
              " (451, 3),\n",
              " (461, 1),\n",
              " (470, 1),\n",
              " (477, 1),\n",
              " (499, 2),\n",
              " (506, 1),\n",
              " (514, 1),\n",
              " (515, 1),\n",
              " (518, 3),\n",
              " (523, 1),\n",
              " (525, 2),\n",
              " (528, 1),\n",
              " (531, 1),\n",
              " (532, 4),\n",
              " (535, 1),\n",
              " (541, 3),\n",
              " (545, 2),\n",
              " (548, 2),\n",
              " (554, 1),\n",
              " (557, 40),\n",
              " (564, 1),\n",
              " (569, 3),\n",
              " (576, 1),\n",
              " (614, 2),\n",
              " (617, 6),\n",
              " (633, 2),\n",
              " (648, 2),\n",
              " (650, 1),\n",
              " (657, 3),\n",
              " (680, 3),\n",
              " (691, 5),\n",
              " (730, 4),\n",
              " (734, 2),\n",
              " (750, 1),\n",
              " (754, 1),\n",
              " (756, 1),\n",
              " (758, 1),\n",
              " (768, 1),\n",
              " (782, 2),\n",
              " (784, 7),\n",
              " (794, 1),\n",
              " (795, 1),\n",
              " (819, 21),\n",
              " (836, 7),\n",
              " (849, 2),\n",
              " (854, 3),\n",
              " (856, 1),\n",
              " (862, 1),\n",
              " (870, 27),\n",
              " (871, 1),\n",
              " (873, 1),\n",
              " (876, 5),\n",
              " (880, 4),\n",
              " (882, 1),\n",
              " (886, 1),\n",
              " (891, 2),\n",
              " (897, 4),\n",
              " (903, 1),\n",
              " (906, 2),\n",
              " (909, 1),\n",
              " (911, 2),\n",
              " (912, 1),\n",
              " (913, 1),\n",
              " (916, 2),\n",
              " (918, 1),\n",
              " (929, 1),\n",
              " (930, 10),\n",
              " (934, 1),\n",
              " (935, 3),\n",
              " (939, 1),\n",
              " (949, 1),\n",
              " (950, 1),\n",
              " (951, 2),\n",
              " (953, 1),\n",
              " (957, 7),\n",
              " (965, 1),\n",
              " (969, 1),\n",
              " (978, 1),\n",
              " (981, 1),\n",
              " (983, 4),\n",
              " (986, 1),\n",
              " (994, 2),\n",
              " (1001, 1),\n",
              " (1002, 2),\n",
              " (1006, 1),\n",
              " (1007, 3),\n",
              " (1012, 12),\n",
              " (1014, 2),\n",
              " (1016, 2),\n",
              " (1018, 13),\n",
              " (1019, 1),\n",
              " (1024, 1),\n",
              " (1026, 2),\n",
              " (1032, 1),\n",
              " (1033, 9),\n",
              " (1036, 1),\n",
              " (1041, 1),\n",
              " (1045, 1),\n",
              " (1046, 1),\n",
              " (1101, 18),\n",
              " (1118, 2),\n",
              " (1135, 2),\n",
              " (1146, 3),\n",
              " (1147, 2),\n",
              " (1155, 1),\n",
              " (1156, 1),\n",
              " (1164, 1),\n",
              " (1173, 1),\n",
              " (1183, 1),\n",
              " (1186, 2),\n",
              " (1198, 2),\n",
              " (1211, 1),\n",
              " (1215, 1),\n",
              " (1217, 4),\n",
              " (1219, 2),\n",
              " (1229, 1),\n",
              " (1233, 2),\n",
              " (1259, 1),\n",
              " (1265, 1),\n",
              " (1267, 1),\n",
              " (1274, 1),\n",
              " (1289, 31),\n",
              " (1302, 3),\n",
              " (1313, 8),\n",
              " (1324, 5),\n",
              " (1354, 2),\n",
              " (1375, 1),\n",
              " (1386, 2),\n",
              " (1401, 1),\n",
              " (1418, 3),\n",
              " (1429, 2),\n",
              " (1437, 4),\n",
              " (1441, 1),\n",
              " (1442, 1),\n",
              " (1446, 1),\n",
              " (1448, 2),\n",
              " (1468, 1),\n",
              " (1469, 1),\n",
              " (1497, 1),\n",
              " (1510, 2),\n",
              " (1548, 1),\n",
              " (1550, 1),\n",
              " (1554, 1),\n",
              " (1561, 2),\n",
              " (1584, 1),\n",
              " (1605, 1),\n",
              " (1622, 1),\n",
              " (1626, 1),\n",
              " (1634, 1),\n",
              " (1649, 3),\n",
              " (1653, 6),\n",
              " (1664, 7),\n",
              " (1667, 1),\n",
              " (1689, 1),\n",
              " (1696, 3),\n",
              " (1724, 1),\n",
              " (1749, 1),\n",
              " (1752, 1),\n",
              " (1771, 2),\n",
              " (1776, 1),\n",
              " (1787, 1),\n",
              " (1798, 1),\n",
              " (1807, 2),\n",
              " (1810, 24),\n",
              " (1812, 2),\n",
              " (1830, 1),\n",
              " (1852, 1),\n",
              " (1859, 1),\n",
              " (1866, 1),\n",
              " (1868, 1),\n",
              " (1933, 1),\n",
              " (1995, 2),\n",
              " (2006, 2),\n",
              " (2026, 1),\n",
              " (2053, 1),\n",
              " (2054, 1),\n",
              " (2075, 1),\n",
              " (2097, 2),\n",
              " (2098, 1),\n",
              " (2105, 1),\n",
              " (2156, 2),\n",
              " (2183, 1),\n",
              " (2221, 2),\n",
              " (2229, 1),\n",
              " (2237, 2),\n",
              " (2258, 3),\n",
              " (2295, 1),\n",
              " (2311, 1),\n",
              " (2378, 1),\n",
              " (2384, 1),\n",
              " (2386, 1),\n",
              " (2404, 2),\n",
              " (2405, 1),\n",
              " (2413, 5),\n",
              " (2458, 2),\n",
              " (2481, 5),\n",
              " (2499, 3),\n",
              " (2558, 1),\n",
              " (2566, 1),\n",
              " (2597, 1),\n",
              " (2634, 1),\n",
              " (2649, 1),\n",
              " (2650, 5),\n",
              " (2666, 1),\n",
              " (2674, 1),\n",
              " (2676, 5),\n",
              " (2702, 2),\n",
              " (2718, 1),\n",
              " (2757, 2),\n",
              " (2780, 1),\n",
              " (2786, 2),\n",
              " (2797, 2),\n",
              " (2801, 1),\n",
              " (2815, 21),\n",
              " (2819, 1),\n",
              " (2831, 4),\n",
              " (2833, 2),\n",
              " (2848, 1),\n",
              " (2861, 2),\n",
              " (2946, 2),\n",
              " (2997, 3),\n",
              " (3025, 1),\n",
              " (3028, 3),\n",
              " (3037, 1),\n",
              " (3054, 1),\n",
              " (3057, 1),\n",
              " (3106, 1),\n",
              " (3112, 6),\n",
              " (3117, 1),\n",
              " (3118, 1),\n",
              " (3150, 1),\n",
              " (3165, 1),\n",
              " (3303, 1),\n",
              " (3355, 2),\n",
              " (3431, 1),\n",
              " (3443, 1),\n",
              " (3459, 1),\n",
              " (3469, 1),\n",
              " (3517, 1),\n",
              " (3533, 1),\n",
              " (3536, 1),\n",
              " (3579, 1),\n",
              " (3585, 9),\n",
              " (3627, 1),\n",
              " (3645, 1),\n",
              " (3664, 8),\n",
              " (3831, 1),\n",
              " (3835, 2),\n",
              " (3840, 1),\n",
              " (3911, 1),\n",
              " (3956, 1),\n",
              " (4061, 1),\n",
              " (4135, 1),\n",
              " (4289, 1),\n",
              " (4362, 1),\n",
              " (4417, 1),\n",
              " (4431, 1),\n",
              " (4483, 1),\n",
              " (4592, 9),\n",
              " (4657, 1),\n",
              " (4695, 1),\n",
              " (4804, 2),\n",
              " (5059, 1),\n",
              " (5257, 1),\n",
              " (5260, 1),\n",
              " (5265, 1),\n",
              " (5266, 1),\n",
              " (5271, 1),\n",
              " (5626, 3),\n",
              " (6112, 1),\n",
              " (6115, 1),\n",
              " (6231, 1),\n",
              " (6287, 1),\n",
              " (6340, 1),\n",
              " (6397, 1),\n",
              " (6419, 1),\n",
              " (6503, 3),\n",
              " (6510, 2),\n",
              " (6526, 1),\n",
              " (6598, 1),\n",
              " (6800, 5),\n",
              " (6889, 3),\n",
              " (7068, 1),\n",
              " (7120, 1),\n",
              " (7414, 1),\n",
              " (7423, 1),\n",
              " (7545, 3),\n",
              " (7554, 1),\n",
              " (7752, 1),\n",
              " (7792, 1),\n",
              " (7797, 2),\n",
              " (8055, 1),\n",
              " (8201, 1),\n",
              " (8417, 8),\n",
              " (8601, 1),\n",
              " (8660, 2),\n",
              " (8859, 1),\n",
              " (8888, 1),\n",
              " (8969, 2),\n",
              " (8970, 1),\n",
              " (9038, 1),\n",
              " (9174, 1),\n",
              " (9248, 1),\n",
              " (9249, 1),\n",
              " (9456, 1),\n",
              " (9618, 1),\n",
              " (9703, 2),\n",
              " (9753, 2),\n",
              " (9967, 1),\n",
              " (10000, 1),\n",
              " (10012, 1),\n",
              " (10301, 1),\n",
              " (10335, 1),\n",
              " (10351, 1),\n",
              " (10430, 1),\n",
              " (10528, 1),\n",
              " (10581, 1),\n",
              " (10686, 1),\n",
              " (10928, 12)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]\n",
        "bow_corpus[4310]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X30EkO6w4kxN",
        "outputId": "32ddb106-cfc6-43c2-981c-2dd025b9633e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 1 (\"acceler\") appears 1 time.\n",
            "Word 4 (\"actual\") appears 1 time.\n",
            "Word 14 (\"appropri\") appears 1 time.\n",
            "Word 16 (\"arbitrarili\") appears 3 time.\n",
            "Word 18 (\"area\") appears 1 time.\n",
            "Word 24 (\"aspect\") appears 2 time.\n",
            "Word 27 (\"attain\") appears 1 time.\n",
            "Word 32 (\"avoid\") appears 2 time.\n",
            "Word 35 (\"binari\") appears 1 time.\n",
            "Word 36 (\"black\") appears 1 time.\n",
            "Word 51 (\"chapter\") appears 1 time.\n",
            "Word 53 (\"check\") appears 1 time.\n",
            "Word 76 (\"correl\") appears 1 time.\n",
            "Word 77 (\"cost\") appears 3 time.\n",
            "Word 93 (\"desir\") appears 2 time.\n",
            "Word 99 (\"display\") appears 1 time.\n",
            "Word 101 (\"distanc\") appears 2 time.\n",
            "Word 104 (\"divid\") appears 1 time.\n",
            "Word 109 (\"easi\") appears 1 time.\n",
            "Word 120 (\"essenti\") appears 1 time.\n",
            "Word 122 (\"euclidean\") appears 3 time.\n",
            "Word 124 (\"excess\") appears 4 time.\n",
            "Word 129 (\"face\") appears 1 time.\n",
            "Word 133 (\"fast\") appears 3 time.\n",
            "Word 134 (\"faster\") appears 2 time.\n",
            "Word 136 (\"finit\") appears 6 time.\n",
            "Word 146 (\"global\") appears 5 time.\n",
            "Word 153 (\"hard\") appears 1 time.\n",
            "Word 169 (\"impli\") appears 2 time.\n",
            "Word 171 (\"impos\") appears 1 time.\n",
            "Word 175 (\"infinit\") appears 1 time.\n",
            "Word 189 (\"knowledg\") appears 1 time.\n",
            "Word 193 (\"lack\") appears 1 time.\n",
            "Word 198 (\"letter\") appears 1 time.\n",
            "Word 205 (\"loss\") appears 18 time.\n",
            "Word 211 (\"mark\") appears 1 time.\n",
            "Word 212 (\"match\") appears 1 time.\n",
            "Word 213 (\"mathemat\") appears 2 time.\n",
            "Word 241 (\"pair\") appears 2 time.\n",
            "Word 244 (\"partial\") appears 2 time.\n",
            "Word 245 (\"pass\") appears 7 time.\n",
            "Word 252 (\"play\") appears 1 time.\n",
            "Word 255 (\"precis\") appears 1 time.\n",
            "Word 261 (\"produc\") appears 1 time.\n",
            "Word 263 (\"project\") appears 4 time.\n",
            "Word 265 (\"proper\") appears 1 time.\n",
            "Word 266 (\"proport\") appears 7 time.\n",
            "Word 269 (\"quick\") appears 1 time.\n",
            "Word 271 (\"ratio\") appears 1 time.\n",
            "Word 278 (\"recurs\") appears 7 time.\n",
            "Word 285 (\"respons\") appears 1 time.\n",
            "Word 291 (\"role\") appears 2 time.\n",
            "Word 297 (\"run\") appears 2 time.\n",
            "Word 302 (\"self\") appears 4 time.\n",
            "Word 305 (\"sequenc\") appears 4 time.\n",
            "Word 307 (\"simplest\") appears 1 time.\n",
            "Word 308 (\"situat\") appears 1 time.\n",
            "Word 310 (\"slight\") appears 4 time.\n",
            "Word 311 (\"slowli\") appears 1 time.\n",
            "Word 317 (\"squar\") appears 37 time.\n",
            "Word 318 (\"stage\") appears 2 time.\n",
            "Word 320 (\"stationari\") appears 5 time.\n",
            "Word 323 (\"strong\") appears 18 time.\n",
            "Word 338 (\"sure\") appears 5 time.\n",
            "Word 348 (\"temporarili\") appears 1 time.\n",
            "Word 364 (\"understand\") appears 1 time.\n",
            "Word 365 (\"uniqu\") appears 1 time.\n",
            "Word 366 (\"unit\") appears 1 time.\n",
            "Word 368 (\"upper\") appears 2 time.\n",
            "Word 379 (\"wide\") appears 1 time.\n",
            "Word 388 (\"advantag\") appears 1 time.\n",
            "Word 392 (\"amount\") appears 1 time.\n",
            "Word 393 (\"analyz\") appears 3 time.\n",
            "Word 400 (\"asymptot\") appears 6 time.\n",
            "Word 402 (\"behavior\") appears 4 time.\n",
            "Word 420 (\"constraint\") appears 1 time.\n",
            "Word 440 (\"earli\") appears 4 time.\n",
            "Word 450 (\"explicit\") appears 1 time.\n",
            "Word 451 (\"extens\") appears 3 time.\n",
            "Word 461 (\"help\") appears 1 time.\n",
            "Word 470 (\"interpret\") appears 1 time.\n",
            "Word 477 (\"later\") appears 1 time.\n",
            "Word 499 (\"nois\") appears 2 time.\n",
            "Word 506 (\"outsid\") appears 1 time.\n",
            "Word 514 (\"potenti\") appears 1 time.\n",
            "Word 515 (\"prefer\") appears 1 time.\n",
            "Word 518 (\"proc\") appears 3 time.\n",
            "Word 523 (\"quantiti\") appears 1 time.\n",
            "Word 525 (\"reach\") appears 2 time.\n",
            "Word 528 (\"recurr\") appears 1 time.\n",
            "Word 531 (\"relax\") appears 1 time.\n",
            "Word 532 (\"replac\") appears 4 time.\n",
            "Word 535 (\"restrict\") appears 1 time.\n",
            "Word 541 (\"satisfi\") appears 3 time.\n",
            "Word 545 (\"signal\") appears 2 time.\n",
            "Word 548 (\"singer\") appears 2 time.\n",
            "Word 554 (\"stabil\") appears 1 time.\n",
            "Word 557 (\"stochast\") appears 40 time.\n",
            "Word 564 (\"thank\") appears 1 time.\n",
            "Word 569 (\"tune\") appears 3 time.\n",
            "Word 576 (\"vari\") appears 1 time.\n",
            "Word 614 (\"context\") appears 2 time.\n",
            "Word 617 (\"covari\") appears 6 time.\n",
            "Word 633 (\"exhibit\") appears 2 time.\n",
            "Word 648 (\"harri\") appears 2 time.\n",
            "Word 650 (\"higher\") appears 1 time.\n",
            "Word 657 (\"ident\") appears 3 time.\n",
            "Word 680 (\"middl\") appears 3 time.\n",
            "Word 691 (\"novel\") appears 5 time.\n",
            "Word 730 (\"smaller\") appears 4 time.\n",
            "Word 734 (\"springer\") appears 2 time.\n",
            "Word 750 (\"transmiss\") appears 1 time.\n",
            "Word 754 (\"univ\") appears 1 time.\n",
            "Word 756 (\"verlag\") appears 1 time.\n",
            "Word 758 (\"weak\") appears 1 time.\n",
            "Word 768 (\"affect\") appears 1 time.\n",
            "Word 782 (\"cambridg\") appears 2 time.\n",
            "Word 784 (\"chain\") appears 7 time.\n",
            "Word 794 (\"determinist\") appears 1 time.\n",
            "Word 795 (\"deviat\") appears 1 time.\n",
            "Word 819 (\"gradient\") appears 21 time.\n",
            "Word 836 (\"markov\") appears 7 time.\n",
            "Word 849 (\"nip\") appears 2 time.\n",
            "Word 854 (\"optimum\") appears 3 time.\n",
            "Word 856 (\"part\") appears 1 time.\n",
            "Word 862 (\"probabilist\") appears 1 time.\n",
            "Word 870 (\"regress\") appears 27 time.\n",
            "Word 871 (\"regular\") appears 1 time.\n",
            "Word 873 (\"reli\") appears 1 time.\n",
            "Word 876 (\"risk\") appears 5 time.\n",
            "Word 880 (\"see\") appears 4 time.\n",
            "Word 882 (\"sequenti\") appears 1 time.\n",
            "Word 886 (\"smallest\") appears 1 time.\n",
            "Word 891 (\"strategi\") appears 2 time.\n",
            "Word 897 (\"technic\") appears 4 time.\n",
            "Word 903 (\"trivial\") appears 1 time.\n",
            "Word 906 (\"uniform\") appears 2 time.\n",
            "Word 909 (\"valid\") appears 1 time.\n",
            "Word 911 (\"variant\") appears 2 time.\n",
            "Word 912 (\"variat\") appears 1 time.\n",
            "Word 913 (\"volum\") appears 1 time.\n",
            "Word 916 (\"wiley\") appears 2 time.\n",
            "Word 918 (\"abl\") appears 1 time.\n",
            "Word 929 (\"bias\") appears 1 time.\n",
            "Word 930 (\"bind\") appears 10 time.\n",
            "Word 934 (\"central\") appears 1 time.\n",
            "Word 935 (\"certain\") appears 3 time.\n",
            "Word 939 (\"classif\") appears 1 time.\n",
            "Word 949 (\"crucial\") appears 1 time.\n",
            "Word 950 (\"curv\") appears 1 time.\n",
            "Word 951 (\"dash\") appears 2 time.\n",
            "Word 953 (\"deal\") appears 1 time.\n",
            "Word 957 (\"descent\") appears 7 time.\n",
            "Word 965 (\"electron\") appears 1 time.\n",
            "Word 969 (\"ensur\") appears 1 time.\n",
            "Word 978 (\"get\") appears 1 time.\n",
            "Word 981 (\"happen\") appears 1 time.\n",
            "Word 983 (\"hold\") appears 4 time.\n",
            "Word 986 (\"instanc\") appears 1 time.\n",
            "Word 994 (\"lowest\") appears 2 time.\n",
            "Word 1001 (\"minimum\") appears 1 time.\n",
            "Word 1002 (\"mixtur\") appears 2 time.\n",
            "Word 1006 (\"overal\") appears 1 time.\n",
            "Word 1007 (\"overfit\") appears 3 time.\n",
            "Word 1012 (\"plot\") appears 12 time.\n",
            "Word 1014 (\"predictor\") appears 2 time.\n",
            "Word 1016 (\"program\") appears 2 time.\n",
            "Word 1018 (\"quadrat\") appears 13 time.\n",
            "Word 1019 (\"quantifi\") appears 1 time.\n",
            "Word 1024 (\"say\") appears 1 time.\n",
            "Word 1026 (\"scheme\") appears 2 time.\n",
            "Word 1032 (\"simpli\") appears 1 time.\n",
            "Word 1033 (\"smooth\") appears 9 time.\n",
            "Word 1036 (\"speech\") appears 1 time.\n",
            "Word 1041 (\"text\") appears 1 time.\n",
            "Word 1045 (\"tradeoff\") appears 1 time.\n",
            "Word 1046 (\"transact\") appears 1 time.\n",
            "Word 1101 (\"dataset\") appears 18 time.\n",
            "Word 1118 (\"explain\") appears 2 time.\n",
            "Word 1135 (\"hessian\") appears 2 time.\n",
            "Word 1146 (\"invari\") appears 3 time.\n",
            "Word 1147 (\"invert\") appears 2 time.\n",
            "Word 1155 (\"logarithm\") appears 1 time.\n",
            "Word 1156 (\"longer\") appears 1 time.\n",
            "Word 1164 (\"notabl\") appears 1 time.\n",
            "Word 1173 (\"perturb\") appears 1 time.\n",
            "Word 1183 (\"qualiti\") appears 1 time.\n",
            "Word 1186 (\"relationship\") appears 2 time.\n",
            "Word 1198 (\"sharp\") appears 2 time.\n",
            "Word 1211 (\"spline\") appears 1 time.\n",
            "Word 1215 (\"sum\") appears 1 time.\n",
            "Word 1217 (\"tabl\") appears 4 time.\n",
            "Word 1219 (\"tend\") appears 2 time.\n",
            "Word 1229 (\"underli\") appears 1 time.\n",
            "Word 1233 (\"way\") appears 2 time.\n",
            "Word 1259 (\"differenti\") appears 1 time.\n",
            "Word 1265 (\"engin\") appears 1 time.\n",
            "Word 1267 (\"exponenti\") appears 1 time.\n",
            "Word 1274 (\"frequent\") appears 1 time.\n",
            "Word 1289 (\"logist\") appears 31 time.\n",
            "Word 1302 (\"preserv\") appears 3 time.\n",
            "Word 1313 (\"spars\") appears 8 time.\n",
            "Word 1324 (\"updat\") appears 5 time.\n",
            "Word 1354 (\"column\") appears 2 time.\n",
            "Word 1375 (\"extra\") appears 1 time.\n",
            "Word 1386 (\"greater\") appears 2 time.\n",
            "Word 1401 (\"kluwer\") appears 1 time.\n",
            "Word 1418 (\"onlin\") appears 3 time.\n",
            "Word 1429 (\"power\") appears 2 time.\n",
            "Word 1437 (\"robust\") appears 4 time.\n",
            "Word 1441 (\"sign\") appears 1 time.\n",
            "Word 1442 (\"slowest\") appears 1 time.\n",
            "Word 1446 (\"strict\") appears 1 time.\n",
            "Word 1448 (\"success\") appears 2 time.\n",
            "Word 1468 (\"year\") appears 1 time.\n",
            "Word 1469 (\"accur\") appears 1 time.\n",
            "Word 1497 (\"earlier\") appears 1 time.\n",
            "Word 1510 (\"go\") appears 2 time.\n",
            "Word 1548 (\"radius\") appears 1 time.\n",
            "Word 1550 (\"residu\") appears 1 time.\n",
            "Word 1554 (\"sensit\") appears 1 time.\n",
            "Word 1561 (\"tail\") appears 2 time.\n",
            "Word 1584 (\"council\") appears 1 time.\n",
            "Word 1605 (\"issu\") appears 1 time.\n",
            "Word 1622 (\"remov\") appears 1 time.\n",
            "Word 1626 (\"scope\") appears 1 time.\n",
            "Word 1634 (\"ubiquit\") appears 1 time.\n",
            "Word 1649 (\"benchmark\") appears 3 time.\n",
            "Word 1653 (\"bound\") appears 6 time.\n",
            "Word 1664 (\"decay\") appears 7 time.\n",
            "Word 1667 (\"definit\") appears 1 time.\n",
            "Word 1689 (\"goal\") appears 1 time.\n",
            "Word 1696 (\"harder\") appears 3 time.\n",
            "Word 1724 (\"loos\") appears 1 time.\n",
            "Word 1749 (\"rank\") appears 1 time.\n",
            "Word 1752 (\"reinforc\") appears 1 time.\n",
            "Word 1771 (\"specifi\") appears 2 time.\n",
            "Word 1776 (\"surpris\") appears 1 time.\n",
            "Word 1787 (\"track\") appears 1 time.\n",
            "Word 1798 (\"wors\") appears 1 time.\n",
            "Word 1807 (\"classic\") appears 2 time.\n",
            "Word 1810 (\"convex\") appears 24 time.\n",
            "Word 1812 (\"diagon\") appears 2 time.\n",
            "Word 1830 (\"inequ\") appears 1 time.\n",
            "Word 1852 (\"parametr\") appears 1 time.\n",
            "Word 1859 (\"proxim\") appears 1 time.\n",
            "Word 1866 (\"stationar\") appears 1 time.\n",
            "Word 1868 (\"subspac\") appears 1 time.\n",
            "Word 1933 (\"ax\") appears 1 time.\n",
            "Word 1995 (\"degre\") appears 2 time.\n",
            "Word 2006 (\"forget\") appears 2 time.\n",
            "Word 2026 (\"matric\") appears 1 time.\n",
            "Word 2053 (\"schmidt\") appears 1 time.\n",
            "Word 2054 (\"semi\") appears 1 time.\n",
            "Word 2075 (\"walk\") appears 1 time.\n",
            "Word 2097 (\"outperform\") appears 2 time.\n",
            "Word 2098 (\"practition\") appears 1 time.\n",
            "Word 2105 (\"supervis\") appears 1 time.\n",
            "Word 2156 (\"trick\") appears 2 time.\n",
            "Word 2183 (\"expans\") appears 1 time.\n",
            "Word 2221 (\"replic\") appears 2 time.\n",
            "Word 2229 (\"share\") appears 1 time.\n",
            "Word 2237 (\"stronger\") appears 2 time.\n",
            "Word 2258 (\"color\") appears 3 time.\n",
            "Word 2295 (\"acoust\") appears 1 time.\n",
            "Word 2311 (\"edit\") appears 1 time.\n",
            "Word 2378 (\"composit\") appears 1 time.\n",
            "Word 2384 (\"entri\") appears 1 time.\n",
            "Word 2386 (\"ergod\") appears 1 time.\n",
            "Word 2404 (\"norm\") appears 2 time.\n",
            "Word 2405 (\"novelti\") appears 1 time.\n",
            "Word 2413 (\"rat\") appears 5 time.\n",
            "Word 2458 (\"homogen\") appears 2 time.\n",
            "Word 2481 (\"proof\") appears 5 time.\n",
            "Word 2499 (\"twice\") appears 3 time.\n",
            "Word 2558 (\"polynomi\") appears 1 time.\n",
            "Word 2566 (\"scenario\") appears 1 time.\n",
            "Word 2597 (\"increment\") appears 1 time.\n",
            "Word 2634 (\"tradit\") appears 1 time.\n",
            "Word 2649 (\"difficulti\") appears 1 time.\n",
            "Word 2650 (\"eigenvalu\") appears 5 time.\n",
            "Word 2666 (\"oscil\") appears 1 time.\n",
            "Word 2674 (\"son\") appears 1 time.\n",
            "Word 2676 (\"synthet\") appears 5 time.\n",
            "Word 2702 (\"doubl\") appears 2 time.\n",
            "Word 2718 (\"mention\") appears 1 time.\n",
            "Word 2757 (\"freedom\") appears 2 time.\n",
            "Word 2780 (\"barrier\") appears 1 time.\n",
            "Word 2786 (\"coincid\") appears 2 time.\n",
            "Word 2797 (\"eric\") appears 2 time.\n",
            "Word 2801 (\"finer\") appears 1 time.\n",
            "Word 2815 (\"newton\") appears 21 time.\n",
            "Word 2819 (\"oppos\") appears 1 time.\n",
            "Word 2831 (\"siam\") appears 4 time.\n",
            "Word 2833 (\"sparsiti\") appears 2 time.\n",
            "Word 2848 (\"behav\") appears 1 time.\n",
            "Word 2861 (\"inferior\") appears 2 time.\n",
            "Word 2946 (\"pari\") appears 2 time.\n",
            "Word 2997 (\"stepsiz\") appears 3 time.\n",
            "Word 3025 (\"lie\") appears 1 time.\n",
            "Word 3028 (\"moment\") appears 3 time.\n",
            "Word 3037 (\"quicker\") appears 1 time.\n",
            "Word 3054 (\"decent\") appears 1 time.\n",
            "Word 3057 (\"european\") appears 1 time.\n",
            "Word 3106 (\"notion\") appears 1 time.\n",
            "Word 3112 (\"theorem\") appears 6 time.\n",
            "Word 3117 (\"annal\") appears 1 time.\n",
            "Word 3118 (\"approx\") appears 1 time.\n",
            "Word 3150 (\"underfit\") appears 1 time.\n",
            "Word 3165 (\"dedic\") appears 1 time.\n",
            "Word 3303 (\"slower\") appears 1 time.\n",
            "Word 3355 (\"plain\") appears 2 time.\n",
            "Word 3431 (\"unregular\") appears 1 time.\n",
            "Word 3443 (\"introductori\") appears 1 time.\n",
            "Word 3459 (\"bertseka\") appears 1 time.\n",
            "Word 3469 (\"polyak\") appears 1 time.\n",
            "Word 3517 (\"telecom\") appears 1 time.\n",
            "Word 3533 (\"kurtosi\") appears 1 time.\n",
            "Word 3536 (\"outlier\") appears 1 time.\n",
            "Word 3579 (\"rescal\") appears 1 time.\n",
            "Word 3585 (\"alpha\") appears 9 time.\n",
            "Word 3627 (\"unbias\") appears 1 time.\n",
            "Word 3645 (\"inspir\") appears 1 time.\n",
            "Word 3664 (\"bach\") appears 8 time.\n",
            "Word 3831 (\"ecol\") appears 1 time.\n",
            "Word 3835 (\"franc\") appears 2 time.\n",
            "Word 3840 (\"inria\") appears 1 time.\n",
            "Word 3911 (\"protocol\") appears 1 time.\n",
            "Word 3956 (\"remedi\") appears 1 time.\n",
            "Word 4061 (\"bottou\") appears 1 time.\n",
            "Word 4135 (\"solver\") appears 1 time.\n",
            "Word 4289 (\"lighter\") appears 1 time.\n",
            "Word 4362 (\"vacuous\") appears 1 time.\n",
            "Word 4417 (\"team\") appears 1 time.\n",
            "Word 4431 (\"lectur\") appears 1 time.\n",
            "Word 4483 (\"west\") appears 1 time.\n",
            "Word 4592 (\"news\") appears 9 time.\n",
            "Word 4657 (\"corollari\") appears 1 time.\n",
            "Word 4695 (\"surrog\") appears 1 time.\n",
            "Word 4804 (\"colt\") appears 2 time.\n",
            "Word 5059 (\"eigenvector\") appears 1 time.\n",
            "Word 5257 (\"kushner\") appears 1 time.\n",
            "Word 5260 (\"monro\") appears 1 time.\n",
            "Word 5265 (\"regret\") appears 1 time.\n",
            "Word 5266 (\"robbin\") appears 1 time.\n",
            "Word 5271 (\"aggreg\") appears 1 time.\n",
            "Word 5626 (\"limn\") appears 3 time.\n",
            "Word 6112 (\"meyn\") appears 1 time.\n",
            "Word 6115 (\"tweedi\") appears 1 time.\n",
            "Word 6231 (\"worsen\") appears 1 time.\n",
            "Word 6287 (\"borkar\") appears 1 time.\n",
            "Word 6340 (\"primal\") appears 1 time.\n",
            "Word 6397 (\"cramer\") appears 1 time.\n",
            "Word 6419 (\"errat\") appears 1 time.\n",
            "Word 6503 (\"franci\") appears 3 time.\n",
            "Word 6510 (\"quantum\") appears 2 time.\n",
            "Word 6526 (\"shapiro\") appears 1 time.\n",
            "Word 6598 (\"anova\") appears 1 time.\n",
            "Word 6800 (\"moulin\") appears 5 time.\n",
            "Word 6889 (\"pointwis\") appears 3 time.\n",
            "Word 7068 (\"icml\") appears 1 time.\n",
            "Word 7120 (\"adjoint\") appears 1 time.\n",
            "Word 7414 (\"sussex\") appears 1 time.\n",
            "Word 7423 (\"kale\") appears 1 time.\n",
            "Word 7545 (\"concord\") appears 3 time.\n",
            "Word 7554 (\"nicola\") appears 1 time.\n",
            "Word 7752 (\"dyadic\") appears 1 time.\n",
            "Word 7792 (\"homoscedast\") appears 1 time.\n",
            "Word 7797 (\"juditski\") appears 2 time.\n",
            "Word 8055 (\"fastica\") appears 1 time.\n",
            "Word 8201 (\"vaart\") appears 1 time.\n",
            "Word 8417 (\"covertyp\") appears 8 time.\n",
            "Word 8601 (\"bousquet\") appears 1 time.\n",
            "Word 8660 (\"subgradi\") appears 2 time.\n",
            "Word 8859 (\"shwartz\") appears 1 time.\n",
            "Word 8888 (\"arinen\") appears 1 time.\n",
            "Word 8969 (\"nemirovski\") appears 2 time.\n",
            "Word 8970 (\"nesterov\") appears 1 time.\n",
            "Word 9038 (\"srebro\") appears 1 time.\n",
            "Word 9174 (\"nonasymptot\") appears 1 time.\n",
            "Word 9248 (\"orfi\") appears 1 time.\n",
            "Word 9249 (\"tsybakov\") appears 1 time.\n",
            "Word 9456 (\"shalev\") appears 1 time.\n",
            "Word 9618 (\"roux\") appears 1 time.\n",
            "Word 9703 (\"sierra\") appears 2 time.\n",
            "Word 9753 (\"hazan\") appears 2 time.\n",
            "Word 9967 (\"paristech\") appears 1 time.\n",
            "Word 10000 (\"yudin\") appears 1 time.\n",
            "Word 10012 (\"supn\") appears 1 time.\n",
            "Word 10301 (\"duchi\") appears 1 time.\n",
            "Word 10335 (\"ltci\") appears 1 time.\n",
            "Word 10351 (\"pegaso\") appears 1 time.\n",
            "Word 10430 (\"erieur\") appears 1 time.\n",
            "Word 10528 (\"pathwis\") appears 1 time.\n",
            "Word 10581 (\"nedic\") appears 1 time.\n",
            "Word 10686 (\"unimprov\") appears 1 time.\n",
            "Word 10928 (\"adagrad\") appears 12 time.\n"
          ]
        }
      ],
      "source": [
        "bow_doc_4310 = bow_corpus[4310]\n",
        "\n",
        "for i in range(len(bow_doc_4310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
        "                                                     dictionary[bow_doc_4310[i][0]], \n",
        "                                                     bow_doc_4310[i][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5qI77BxBVbY"
      },
      "source": [
        "**TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2IG161ao4k0l"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora, models\n",
        "tfidf = models.TfidfModel(bow_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U3kTYib79Hf-"
      },
      "outputs": [],
      "source": [
        "corpus_tfidf = tfidf[bow_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhvs2LvS9Hii",
        "outputId": "eb701cc3-959b-46ff-bf98-5df4257d11b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.04645512099680736),\n",
            " (1, 0.016194928210910717),\n",
            " (2, 0.03096844659650005),\n",
            " (3, 0.018426259160709415),\n",
            " (4, 0.020461159529786836),\n",
            " (5, 0.01376512754213418),\n",
            " (6, 0.01659453474408978),\n",
            " (7, 0.012641218904195561),\n",
            " (8, 0.01335711487765653),\n",
            " (9, 0.02461038396068751),\n",
            " (10, 0.010087372820476804),\n",
            " (11, 0.016343478223700688),\n",
            " (12, 0.02661966195179876),\n",
            " (13, 0.02849254213874603),\n",
            " (14, 0.0068436445356545505),\n",
            " (15, 0.007354451089734281),\n",
            " (16, 0.014485476057756104),\n",
            " (17, 0.01936239523311201),\n",
            " (18, 0.06139049065156623),\n",
            " (19, 0.0468978820085069),\n",
            " (20, 0.01002468186537162),\n",
            " (21, 0.018577807583822378),\n",
            " (22, 0.016540047704476257),\n",
            " (23, 0.014027450397921304),\n",
            " (24, 0.01076531620187347),\n",
            " (25, 0.007584525635881085),\n",
            " (26, 0.027764036850892904),\n",
            " (27, 0.04983834778309642),\n",
            " (28, 0.016662733045685118),\n",
            " (29, 0.07570783072618265),\n",
            " (30, 0.009854852845925325),\n",
            " (31, 0.1138491149216239),\n",
            " (32, 0.03202349391535267),\n",
            " (33, 0.013782600583098046),\n",
            " (34, 0.011641809939569392),\n",
            " (35, 0.021449513391755593),\n",
            " (36, 0.11461301941929262),\n",
            " (37, 0.07748412318484771),\n",
            " (38, 0.023445670146311853),\n",
            " (39, 0.02081410521667381),\n",
            " (40, 0.0322940665541428),\n",
            " (41, 0.020906642051772985),\n",
            " (42, 0.007310344443253011),\n",
            " (43, 0.023185268857904066),\n",
            " (44, 0.253641068817111),\n",
            " (45, 0.05608306475253168),\n",
            " (46, 0.01684458351877672),\n",
            " (47, 0.029777119991505098),\n",
            " (48, 0.030135094326590256),\n",
            " (49, 0.01359472858873692),\n",
            " (50, 0.007160615133056182),\n",
            " (51, 0.02089306827413847),\n",
            " (52, 0.05911056447471657),\n",
            " (53, 0.015009504669843875),\n",
            " (54, 0.02727857336688393),\n",
            " (55, 0.017647144650975943),\n",
            " (56, 0.026074154834935394),\n",
            " (57, 0.008448280725832789),\n",
            " (58, 0.03125970449753233),\n",
            " (59, 0.03335398146458598),\n",
            " (60, 0.02073554587609526),\n",
            " (61, 0.007147145804571493),\n",
            " (62, 0.06301127309140815),\n",
            " (63, 0.017322629791490688),\n",
            " (64, 0.0685973447355112),\n",
            " (65, 0.029138439343730428),\n",
            " (66, 0.0645383161323092),\n",
            " (67, 0.047367492991143824),\n",
            " (68, 0.022997773804926187),\n",
            " (69, 0.05303558111713225),\n",
            " (70, 0.0141926174601692),\n",
            " (71, 0.008941208373744552),\n",
            " (72, 0.019325109011215983),\n",
            " (73, 0.02448318555483227),\n",
            " (74, 0.14360226556613562),\n",
            " (75, 0.02175644758619009),\n",
            " (76, 0.007673316578339303),\n",
            " (77, 0.025604109236388985),\n",
            " (78, 0.048372339834171045),\n",
            " (79, 0.013650613303864408),\n",
            " (80, 0.011970858454632175),\n",
            " (81, 0.023871638002025398),\n",
            " (82, 0.035762551430457236),\n",
            " (83, 0.008486580971275227),\n",
            " (84, 0.03808983391918081),\n",
            " (85, 0.03132045938826996),\n",
            " (86, 0.24091253821468003),\n",
            " (87, 0.013161597888555494),\n",
            " (88, 0.013038509462761983),\n",
            " (89, 0.07617966783836162),\n",
            " (90, 0.034340239825061965),\n",
            " (91, 0.038227845447256874),\n",
            " (92, 0.04808365289004104),\n",
            " (93, 0.014686801722483014),\n",
            " (94, 0.009263364088507559),\n",
            " (95, 0.0067561449005604695),\n",
            " (96, 0.01758854096706055),\n",
            " (97, 0.03317193422033821),\n",
            " (98, 0.007112234767778987),\n",
            " (99, 0.014423257178191264),\n",
            " (100, 0.024089642700926128),\n",
            " (101, 0.013334184827505592),\n",
            " (102, 0.024584776635351403),\n",
            " (103, 0.012897039430416403),\n",
            " (104, 0.010620682070764147),\n",
            " (105, 0.04467473170077752),\n",
            " (106, 0.030962878571377368),\n",
            " (107, 0.03138169455980633),\n",
            " (108, 0.013283671519893677),\n",
            " (109, 0.008664588172303582),\n",
            " (110, 0.0066544537931324255),\n",
            " (111, 0.04213934686246568),\n",
            " (112, 0.05803228904086086),\n",
            " (113, 0.017595382900447064),\n",
            " (114, 0.02421049084400299),\n",
            " (115, 0.009217517965525321),\n",
            " (116, 0.047367492991143824),\n",
            " (117, 0.06987812278894057),\n",
            " (118, 0.025941789132952836),\n",
            " (119, 0.010384106542116968),\n",
            " (120, 0.00829694582493821),\n",
            " (121, 0.05938177387998372),\n",
            " (122, 0.02827866584856945),\n",
            " (123, 0.01892695768154566),\n",
            " (124, 0.06572793160704027),\n",
            " (125, 0.021927430330748943),\n",
            " (126, 0.020720399934556488),\n",
            " (127, 0.01645902909288382),\n",
            " (128, 0.005475900777155816),\n",
            " (129, 0.012489540038389958),\n",
            " (130, 0.018708398105755943),\n",
            " (131, 0.02764927610212525),\n",
            " (132, 0.014341068514749629),\n",
            " (133, 0.006997868061673282),\n",
            " (134, 0.009584739746453831),\n",
            " (135, 0.019924988409694925),\n",
            " (136, 0.03679991060464755),\n",
            " (137, 0.016238324452476725),\n",
            " (138, 0.03615935634609923),\n",
            " (139, 0.015872829362203786),\n",
            " (140, 0.01842800510827528),\n",
            " (141, 0.03313535820341334),\n",
            " (142, 0.023019949735017906),\n",
            " (143, 0.012934616123670433),\n",
            " (144, 0.032793995418377506),\n",
            " (145, 0.03982093450123227),\n",
            " (146, 0.02295091551691401),\n",
            " (147, 0.04066704217717456),\n",
            " (148, 0.019528534528041142),\n",
            " (149, 0.010323032305683912),\n",
            " (150, 0.006738252313156624),\n",
            " (151, 0.02646425063060505),\n",
            " (152, 0.040477854307666164),\n",
            " (153, 0.0087966665444789),\n",
            " (154, 0.06440869964375207),\n",
            " (155, 0.04098195830439138),\n",
            " (156, 0.04197779776861323),\n",
            " (157, 0.025140854095140077),\n",
            " (158, 0.02866197042751736),\n",
            " (159, 0.04526103763498415),\n",
            " (160, 0.02623064922535046),\n",
            " (161, 0.016074696391377416),\n",
            " (162, 0.026326086270674214),\n",
            " (163, 0.00957368689008181),\n",
            " (164, 0.012751408654978641),\n",
            " (165, 0.13110173692216798),\n",
            " (166, 0.02333316696516774),\n",
            " (167, 0.14694274707395796),\n",
            " (168, 0.02731485544192585),\n",
            " (169, 0.006649404113123216),\n",
            " (170, 0.024235468837489515),\n",
            " (171, 0.01227788181977211),\n",
            " (172, 0.018426259160709415),\n",
            " (173, 0.03648743709264792),\n",
            " (174, 0.03132045938826996),\n",
            " (175, 0.03445421571553568),\n",
            " (176, 0.016741735541476666),\n",
            " (177, 0.008020923432785116),\n",
            " (178, 0.02324955311779305),\n",
            " (179, 0.013757286152530334),\n",
            " (180, 0.01929259114516039),\n",
            " (181, 0.026890299394797906),\n",
            " (182, 0.00941336452113571),\n",
            " (183, 0.021825404699993174),\n",
            " (184, 0.007806975411095122),\n",
            " (185, 0.02489779735924498),\n",
            " (186, 0.04563897643694485),\n",
            " (187, 0.02522400172016377),\n",
            " (188, 0.025195156350943736),\n",
            " (189, 0.011463410939650132),\n",
            " (190, 0.03725908764178105),\n",
            " (191, 0.006872166173720983),\n",
            " (192, 0.014746648612043936),\n",
            " (193, 0.013155749485967521),\n",
            " (194, 0.028618815948996883),\n",
            " (195, 0.08691293098157439),\n",
            " (196, 0.02378398631016881),\n",
            " (197, 0.008117941798273137),\n",
            " (198, 0.1947381029543602),\n",
            " (199, 0.007373827001933797),\n",
            " (200, 0.008531505257936207),\n",
            " (201, 0.02106967343123284),\n",
            " (202, 0.005707082331779772),\n",
            " (203, 0.015212670894658463),\n",
            " (204, 0.0424981590241747),\n",
            " (205, 0.00700050866468498),\n",
            " (206, 0.0688616795838005),\n",
            " (207, 0.02095682343947038),\n",
            " (208, 0.019901778790792756),\n",
            " (209, 0.03424492250154886),\n",
            " (210, 0.024793497394802973),\n",
            " (211, 0.013273564819571464),\n",
            " (212, 0.014075143871358942),\n",
            " (213, 0.006895580424877477),\n",
            " (214, 0.031568359227347714),\n",
            " (215, 0.030847746110855095),\n",
            " (216, 0.0322691580661546),\n",
            " (217, 0.13360023096422025),\n",
            " (218, 0.014395763950182393),\n",
            " (219, 0.08037348195688707),\n",
            " (220, 0.06216049523534005),\n",
            " (221, 0.02358464153939847),\n",
            " (222, 0.07421719434372125),\n",
            " (223, 0.06980090154907437),\n",
            " (224, 0.04963669673355857),\n",
            " (225, 0.007661802330422432),\n",
            " (226, 0.007809906805211396),\n",
            " (227, 0.012189649341132779),\n",
            " (228, 0.006047454090240341),\n",
            " (229, 0.0388981710711509),\n",
            " (230, 0.06784636740653494),\n",
            " (231, 0.012143341744080863),\n",
            " (232, 0.024978069671992027),\n",
            " (233, 0.014681281530826672),\n",
            " (234, 0.01116626153950313),\n",
            " (235, 0.1892047399954708),\n",
            " (236, 0.009341573378356621),\n",
            " (237, 0.06477059880651459),\n",
            " (238, 0.10474562238210733),\n",
            " (239, 0.014604381824206045),\n",
            " (240, 0.08911270248130905),\n",
            " (241, 0.005413248069982572),\n",
            " (242, 0.023970650977710194),\n",
            " (243, 0.010009088082833571),\n",
            " (244, 0.008120993288901453),\n",
            " (245, 0.009974116425872141),\n",
            " (246, 0.0368748993769079),\n",
            " (247, 0.040580904237458366),\n",
            " (248, 0.01747167338963999),\n",
            " (249, 0.027679724693435687),\n",
            " (250, 0.011442447972571324),\n",
            " (251, 0.017780959818293925),\n",
            " (252, 0.011293924139444366),\n",
            " (253, 0.04563897643694485),\n",
            " (254, 0.03348814991596601),\n",
            " (255, 0.02144143741371448),\n",
            " (256, 0.021151299148169548),\n",
            " (257, 0.03521099653293449),\n",
            " (258, 0.01398164383800428),\n",
            " (259, 0.00799685752182444),\n",
            " (260, 0.02553559143245922),\n",
            " (261, 0.01634325719437088),\n",
            " (262, 0.006511812416451606),\n",
            " (263, 0.013620143965252459),\n",
            " (264, 0.01089958429629452),\n",
            " (265, 0.05340446815407385),\n",
            " (266, 0.009496755358686966),\n",
            " (267, 0.014266487371912996),\n",
            " (268, 0.038280024426080514),\n",
            " (269, 0.012478818640332635),\n",
            " (270, 0.017794695947621272),\n",
            " (271, 0.009759992379784357),\n",
            " (272, 0.06345812533483305),\n",
            " (273, 0.03405102237697048),\n",
            " (274, 0.050832284017794306),\n",
            " (275, 0.10638206208464672),\n",
            " (276, 0.08175471965658158),\n",
            " (277, 0.025195156350943736),\n",
            " (278, 0.07109698905668474),\n",
            " (279, 0.02098224563862438),\n",
            " (280, 0.11385135562496364),\n",
            " (281, 0.019771890945645586),\n",
            " (282, 0.015415924741676629),\n",
            " (283, 0.01598994490196798),\n",
            " (284, 0.07461367115365306),\n",
            " (285, 0.008609129712372942),\n",
            " (286, 0.010595436392166674),\n",
            " (287, 0.00986249210577505),\n",
            " (288, 0.030517103663029812),\n",
            " (289, 0.024687717802929172),\n",
            " (290, 0.47244040458460673),\n",
            " (291, 0.019840055929390553),\n",
            " (292, 0.0424977881270482),\n",
            " (293, 0.01110769730871408),\n",
            " (294, 0.1439658212533441),\n",
            " (295, 0.04297987086922595),\n",
            " (296, 0.02264941316825871),\n",
            " (297, 0.007302102333385353),\n",
            " (298, 0.06486728332221627),\n",
            " (299, 0.08866771168601092),\n",
            " (300, 0.0355043571577941),\n",
            " (301, 0.028913261377072137),\n",
            " (302, 0.14388131294017298),\n",
            " (303, 0.03348814991596601),\n",
            " (304, 0.015526274200408705),\n",
            " (305, 0.02238454008150115),\n",
            " (306, 0.012592140331191123),\n",
            " (307, 0.015931165799314157),\n",
            " (308, 0.010314925335964549),\n",
            " (309, 0.07906284209302189),\n",
            " (310, 0.008185353851459971),\n",
            " (311, 0.018577807583822378),\n",
            " (312, 0.0424981590241747),\n",
            " (313, 0.01014660998547707),\n",
            " (314, 0.017773444332481545),\n",
            " (315, 0.030285734953889212),\n",
            " (316, 0.03443083979190025),\n",
            " (317, 0.00560495256129302),\n",
            " (318, 0.039963591920998964),\n",
            " (319, 0.017826683090912787),\n",
            " (320, 0.014987256089130657),\n",
            " (321, 0.0590623758866667),\n",
            " (322, 0.027053936158305578),\n",
            " (323, 0.006656979868113169),\n",
            " (324, 0.058705700112809074),\n",
            " (325, 0.029387291380825086),\n",
            " (326, 0.04633450988988755),\n",
            " (327, 0.011049572525066),\n",
            " (328, 0.006357153132813939),\n",
            " (329, 0.01527390606619483),\n",
            " (330, 0.021625014082616273),\n",
            " (331, 0.016835177070917962),\n",
            " (332, 0.005920065515249117),\n",
            " (333, 0.011193441247598206),\n",
            " (334, 0.03725008142204035),\n",
            " (335, 0.027959165779549433),\n",
            " (336, 0.013644383943770103),\n",
            " (337, 0.036133461132951676),\n",
            " (338, 0.01903859432909743),\n",
            " (339, 0.022129596722214633),\n",
            " (340, 0.01659453474408978),\n",
            " (341, 0.034162158215307104),\n",
            " (342, 0.01701588258704686),\n",
            " (343, 0.030621078616654484),\n",
            " (344, 0.02131717719673976),\n",
            " (345, 0.033902484441232776),\n",
            " (346, 0.10797710765528043),\n",
            " (347, 0.025681475383693315),\n",
            " (348, 0.03373408196413776),\n",
            " (349, 0.02514115377684942),\n",
            " (350, 0.04331430358403721),\n",
            " (351, 0.01610885833807972),\n",
            " (352, 0.034340239825061965),\n",
            " (353, 0.019113922723628437),\n",
            " (354, 0.014942949721285645),\n",
            " (355, 0.014668078738387515),\n",
            " (356, 0.015931165799314157),\n",
            " (357, 0.012393577517609701),\n",
            " (358, 0.05447608271895487),\n",
            " (359, 0.038005859226951086),\n",
            " (360, 0.029339913808914028),\n",
            " (361, 0.018054369721695603),\n",
            " (362, 0.031021340802536955),\n",
            " (363, 0.014764291647549613),\n",
            " (364, 0.0070508519129759825),\n",
            " (365, 0.009330861864052672),\n",
            " (366, 0.011148269960848993),\n",
            " (367, 0.016625145277017313),\n",
            " (368, 0.007365517104273124),\n",
            " (369, 0.01318313633438673),\n",
            " (370, 0.056897125494086064),\n",
            " (371, 0.024587114242592575),\n",
            " (372, 0.030135094326590256),\n",
            " (373, 0.017543445527211904),\n",
            " (374, 0.019422240474962934),\n",
            " (375, 0.016625145277017313),\n",
            " (376, 0.02453381455114333),\n",
            " (377, 0.012548774458042417),\n",
            " (378, 0.04485094961876033),\n",
            " (379, 0.007011080087492298),\n",
            " (380, 0.03808983391918081),\n",
            " (381, 0.030410100845913994),\n",
            " (382, 0.16886250813687922),\n",
            " (383, 0.03869928858607272),\n",
            " (384, 0.032213467297135756)]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for cor in corpus_tfidf:\n",
        "    pprint(cor)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuikC2DPBJjf"
      },
      "source": [
        "**Performing LDA using BOW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rZ-AoaxI9HkU"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHZSVy6s9Hnt",
        "outputId": "37cb0c33-6c0d-4ce7-fa9b-8ddda02a59a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.009*\"gradient\" + 0.009*\"infer\" + 0.008*\"variat\" + 0.007*\"layer\" + 0.006*\"word\" + 0.006*\"cluster\" + 0.006*\"stochast\" + 0.006*\"latent\" + 0.006*\"dataset\" + 0.006*\"likelihood\"\n",
            "Topic: 1 \n",
            "Words: 0.020*\"polici\" + 0.017*\"cluster\" + 0.010*\"action\" + 0.008*\"graph\" + 0.007*\"reward\" + 0.006*\"tree\" + 0.006*\"agent\" + 0.005*\"cost\" + 0.005*\"queri\" + 0.005*\"theorem\"\n",
            "Topic: 2 \n",
            "Words: 0.016*\"tree\" + 0.009*\"sequenc\" + 0.008*\"node\" + 0.007*\"graph\" + 0.006*\"nod\" + 0.006*\"target\" + 0.005*\"label\" + 0.005*\"loss\" + 0.005*\"dataset\" + 0.004*\"detect\"\n",
            "Topic: 3 \n",
            "Words: 0.011*\"bind\" + 0.011*\"loss\" + 0.011*\"regret\" + 0.010*\"theorem\" + 0.010*\"bound\" + 0.009*\"convex\" + 0.008*\"gradient\" + 0.006*\"onlin\" + 0.006*\"proof\" + 0.006*\"lemma\"\n",
            "Topic: 4 \n",
            "Words: 0.025*\"kernel\" + 0.010*\"theorem\" + 0.009*\"loss\" + 0.008*\"norm\" + 0.007*\"regular\" + 0.007*\"regress\" + 0.007*\"rank\" + 0.007*\"bound\" + 0.006*\"label\" + 0.006*\"risk\"\n",
            "Topic: 5 \n",
            "Words: 0.018*\"neuron\" + 0.016*\"activ\" + 0.013*\"spike\" + 0.010*\"layer\" + 0.008*\"unit\" + 0.006*\"signal\" + 0.006*\"synapt\" + 0.005*\"hide\" + 0.005*\"nois\" + 0.005*\"memori\"\n",
            "Topic: 6 \n",
            "Words: 0.014*\"neuron\" + 0.012*\"unit\" + 0.011*\"cell\" + 0.009*\"activ\" + 0.008*\"respons\" + 0.007*\"field\" + 0.007*\"visual\" + 0.006*\"imag\" + 0.006*\"dynam\" + 0.006*\"signal\"\n",
            "Topic: 7 \n",
            "Words: 0.035*\"imag\" + 0.008*\"dataset\" + 0.008*\"layer\" + 0.007*\"nois\" + 0.006*\"convolut\" + 0.006*\"deep\" + 0.005*\"infer\" + 0.005*\"prior\" + 0.005*\"visual\" + 0.005*\"filter\"\n",
            "Topic: 8 \n",
            "Words: 0.008*\"imag\" + 0.007*\"action\" + 0.007*\"layer\" + 0.007*\"agent\" + 0.006*\"sequenc\" + 0.005*\"human\" + 0.005*\"reward\" + 0.005*\"architectur\" + 0.005*\"attent\" + 0.004*\"label\"\n",
            "Topic: 9 \n",
            "Words: 0.013*\"rank\" + 0.010*\"user\" + 0.010*\"graph\" + 0.007*\"item\" + 0.005*\"game\" + 0.005*\"infer\" + 0.005*\"player\" + 0.004*\"edg\" + 0.004*\"program\" + 0.004*\"classifi\"\n"
          ]
        }
      ],
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfpgGmE_AwCG"
      },
      "source": [
        "**Evaluating Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhZ5JgdV9vTN",
        "outputId": "7329438e-c32b-4604-c68b-0fb1254fbedd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['strong',\n",
              " 'convex',\n",
              " 'smooth',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'converg',\n",
              " 'rate',\n",
              " 'eric',\n",
              " 'moulin',\n",
              " 'ltci',\n",
              " 'telecom',\n",
              " 'paristech',\n",
              " 'pari',\n",
              " 'franc',\n",
              " 'eric',\n",
              " 'moulin',\n",
              " 'enst',\n",
              " 'franci',\n",
              " 'bach',\n",
              " 'inria',\n",
              " 'sierra',\n",
              " 'project',\n",
              " 'team',\n",
              " 'ecol',\n",
              " 'normal',\n",
              " 'erieur',\n",
              " 'pari',\n",
              " 'franc',\n",
              " 'franci',\n",
              " 'bach',\n",
              " 'abstract',\n",
              " 'consid',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'problem',\n",
              " 'convex',\n",
              " 'function',\n",
              " 'minim',\n",
              " 'give',\n",
              " 'knowledg',\n",
              " 'unbias',\n",
              " 'estim',\n",
              " 'gradient',\n",
              " 'certain',\n",
              " 'point',\n",
              " 'framework',\n",
              " 'includ',\n",
              " 'machin',\n",
              " 'learn',\n",
              " 'method',\n",
              " 'base',\n",
              " 'minim',\n",
              " 'empir',\n",
              " 'risk',\n",
              " 'focus',\n",
              " 'problem',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'previous',\n",
              " 'know',\n",
              " 'algorithm',\n",
              " 'achiev',\n",
              " 'converg',\n",
              " 'rate',\n",
              " 'function',\n",
              " 'valu',\n",
              " 'iter',\n",
              " 'consid',\n",
              " 'analyz',\n",
              " 'algorithm',\n",
              " 'achiev',\n",
              " 'rate',\n",
              " 'classic',\n",
              " 'supervis',\n",
              " 'learn',\n",
              " 'problem',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'averag',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'descent',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'achiev',\n",
              " 'desir',\n",
              " 'rate',\n",
              " 'logist',\n",
              " 'regress',\n",
              " 'achiev',\n",
              " 'simpl',\n",
              " 'novel',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'algorithm',\n",
              " 'construct',\n",
              " 'success',\n",
              " 'local',\n",
              " 'quadrat',\n",
              " 'approxim',\n",
              " 'loss',\n",
              " 'function',\n",
              " 'preserv',\n",
              " 'run',\n",
              " 'time',\n",
              " 'complex',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'descent',\n",
              " 'algorithm',\n",
              " 'provid',\n",
              " 'asymptot',\n",
              " 'analysi',\n",
              " 'general',\n",
              " 'error',\n",
              " 'expect',\n",
              " 'high',\n",
              " 'probabl',\n",
              " 'squar',\n",
              " 'extens',\n",
              " 'experi',\n",
              " 'show',\n",
              " 'outperform',\n",
              " 'exist',\n",
              " 'approach',\n",
              " 'introduct',\n",
              " 'larg',\n",
              " 'scale',\n",
              " 'machin',\n",
              " 'learn',\n",
              " 'problem',\n",
              " 'ubiquit',\n",
              " 'area',\n",
              " 'scienc',\n",
              " 'engin',\n",
              " 'face',\n",
              " 'larg',\n",
              " 'amount',\n",
              " 'data',\n",
              " 'practition',\n",
              " 'typic',\n",
              " 'prefer',\n",
              " 'algorithm',\n",
              " 'process',\n",
              " 'observ',\n",
              " 'time',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'algorithm',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'descent',\n",
              " 'variant',\n",
              " 'introduc',\n",
              " 'year',\n",
              " 'remain',\n",
              " 'wide',\n",
              " 'studi',\n",
              " 'method',\n",
              " 'context',\n",
              " 'convex',\n",
              " 'function',\n",
              " 'defin',\n",
              " 'euclidean',\n",
              " 'space',\n",
              " 'give',\n",
              " 'consid',\n",
              " 'minim',\n",
              " 'denot',\n",
              " 'data',\n",
              " 'denot',\n",
              " 'loss',\n",
              " 'function',\n",
              " 'convex',\n",
              " 'respect',\n",
              " 'second',\n",
              " 'variabl',\n",
              " 'includ',\n",
              " 'logist',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'framework',\n",
              " 'independ',\n",
              " 'ident',\n",
              " 'distribut',\n",
              " 'pair',\n",
              " 'observ',\n",
              " 'sequenti',\n",
              " 'predictor',\n",
              " 'defin',\n",
              " 'updat',\n",
              " 'pair',\n",
              " 'see',\n",
              " 'partial',\n",
              " 'understand',\n",
              " 'properti',\n",
              " 'affect',\n",
              " 'problem',\n",
              " 'difficulti',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'twice',\n",
              " 'differenti',\n",
              " 'uniform',\n",
              " 'strict',\n",
              " 'posit',\n",
              " 'lower',\n",
              " 'bind',\n",
              " 'hessian',\n",
              " 'properti',\n",
              " 'observ',\n",
              " 'proper',\n",
              " 'step',\n",
              " 'size',\n",
              " 'averag',\n",
              " 'achiev',\n",
              " 'rate',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'case',\n",
              " 'achiev',\n",
              " 'nonstrong',\n",
              " 'convex',\n",
              " 'case',\n",
              " 'match',\n",
              " 'lower',\n",
              " 'bound',\n",
              " 'main',\n",
              " 'issu',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'typic',\n",
              " 'machin',\n",
              " 'learn',\n",
              " 'problem',\n",
              " 'high',\n",
              " 'dimension',\n",
              " 'correl',\n",
              " 'variabl',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'constant',\n",
              " 'zero',\n",
              " 'close',\n",
              " 'zero',\n",
              " 'case',\n",
              " 'smaller',\n",
              " 'make',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'method',\n",
              " 'better',\n",
              " 'paper',\n",
              " 'obtain',\n",
              " 'algorithm',\n",
              " 'deal',\n",
              " 'arbitrarili',\n",
              " 'small',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'constant',\n",
              " 'achiev',\n",
              " 'rate',\n",
              " 'smooth',\n",
              " 'play',\n",
              " 'central',\n",
              " 'role',\n",
              " 'context',\n",
              " 'determinist',\n",
              " 'optim',\n",
              " 'know',\n",
              " 'converg',\n",
              " 'rat',\n",
              " 'smooth',\n",
              " 'optim',\n",
              " 'better',\n",
              " 'smooth',\n",
              " 'optim',\n",
              " 'stochast',\n",
              " 'optim',\n",
              " 'smooth',\n",
              " 'lead',\n",
              " 'improv',\n",
              " 'constant',\n",
              " 'rate',\n",
              " 'remain',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'problem',\n",
              " 'squar',\n",
              " 'loss',\n",
              " 'logist',\n",
              " 'loss',\n",
              " 'smooth',\n",
              " 'loss',\n",
              " 'obtain',\n",
              " 'algorithm',\n",
              " 'converg',\n",
              " 'rate',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'assumpt',\n",
              " 'precis',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'section',\n",
              " 'averag',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'descent',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'achiev',\n",
              " 'desir',\n",
              " 'rate',\n",
              " 'logist',\n",
              " 'regress',\n",
              " 'achiev',\n",
              " 'novel',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'algorithm',\n",
              " 'construct',\n",
              " 'success',\n",
              " 'local',\n",
              " 'quadrat',\n",
              " 'approxim',\n",
              " 'loss',\n",
              " 'function',\n",
              " 'preserv',\n",
              " 'run',\n",
              " 'time',\n",
              " 'complex',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'descent',\n",
              " 'section',\n",
              " 'algorithm',\n",
              " 'provid',\n",
              " 'asymptot',\n",
              " 'analysi',\n",
              " 'general',\n",
              " 'error',\n",
              " 'expect',\n",
              " 'high',\n",
              " 'probabl',\n",
              " 'squar',\n",
              " 'extens',\n",
              " 'experi',\n",
              " 'standard',\n",
              " 'machin',\n",
              " 'learn',\n",
              " 'benchmark',\n",
              " 'show',\n",
              " 'section',\n",
              " 'outperform',\n",
              " 'exist',\n",
              " 'approach',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'mean',\n",
              " 'squar',\n",
              " 'algorithm',\n",
              " 'section',\n",
              " 'consid',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'refer',\n",
              " 'mean',\n",
              " 'squar',\n",
              " 'algorithm',\n",
              " 'novelti',\n",
              " 'converg',\n",
              " 'result',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'averag',\n",
              " 'consid',\n",
              " 'explicit',\n",
              " 'asymptot',\n",
              " 'rate',\n",
              " 'depend',\n",
              " 'lowest',\n",
              " 'eigenvalu',\n",
              " 'covari',\n",
              " 'matrix',\n",
              " 'converg',\n",
              " 'expect',\n",
              " 'follow',\n",
              " 'assumpt',\n",
              " 'dimension',\n",
              " 'euclidean',\n",
              " 'space',\n",
              " 'observ',\n",
              " 'independ',\n",
              " 'ident',\n",
              " 'distribut',\n",
              " 'ekxn',\n",
              " 'ekzn',\n",
              " 'finit',\n",
              " 'denot',\n",
              " 'covari',\n",
              " 'oper',\n",
              " 'loss',\n",
              " 'general',\n",
              " 'assum',\n",
              " 'invert',\n",
              " 'project',\n",
              " 'minim',\n",
              " 'subspac',\n",
              " 'lie',\n",
              " 'sure',\n",
              " 'eigenvalu',\n",
              " 'arbitrarili',\n",
              " 'small',\n",
              " 'global',\n",
              " 'minimum',\n",
              " 'attain',\n",
              " 'certain',\n",
              " 'denot',\n",
              " 'residu',\n",
              " 'general',\n",
              " 'true',\n",
              " 'model',\n",
              " 'specifi',\n",
              " 'studi',\n",
              " 'stochast',\n",
              " 'gradient',\n",
              " 'mean',\n",
              " 'squar',\n",
              " 'recurs',\n",
              " 'defin',\n",
              " 'start',\n",
              " 'consid',\n",
              " 'averag',\n",
              " 'iter',\n",
              " 'exist',\n",
              " 'denot',\n",
              " 'order',\n",
              " 'self',\n",
              " 'adjoint',\n",
              " 'oper',\n",
              " 'posit',\n",
              " 'semi',\n",
              " 'definit',\n",
              " 'discuss',\n",
              " 'assumpt',\n",
              " 'assumpt',\n",
              " 'standard',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'note',\n",
              " 'squar',\n",
              " 'problem',\n",
              " 'form',\n",
              " 'respons',\n",
              " 'predict',\n",
              " 'linear',\n",
              " 'function',\n",
              " 'consid',\n",
              " 'slight',\n",
              " 'general',\n",
              " 'case',\n",
              " 'squar',\n",
              " 'need',\n",
              " 'quadrat',\n",
              " 'approxim',\n",
              " 'logist',\n",
              " 'loss',\n",
              " 'section',\n",
              " 'note',\n",
              " 'assumpt',\n",
              " 'assum',\n",
              " 'model',\n",
              " 'specifi',\n",
              " 'assumpt',\n",
              " 'true',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'bound',\n",
              " 'data',\n",
              " 'sure',\n",
              " 'sure',\n",
              " 'similar',\n",
              " 'inequ',\n",
              " 'hold',\n",
              " 'output',\n",
              " 'variabl',\n",
              " 'hold',\n",
              " 'data',\n",
              " 'infinit',\n",
              " 'support',\n",
              " 'gaussian',\n",
              " 'mixtur',\n",
              " 'gaussian',\n",
              " 'covari',\n",
              " 'matric',\n",
              " 'mixtur',\n",
              " 'compon',\n",
              " 'lower',\n",
              " 'upper',\n",
              " 'bound',\n",
              " 'constant',\n",
              " 'time',\n",
              " 'matrix',\n",
              " 'note',\n",
              " 'finit',\n",
              " 'dimension',\n",
              " 'assumpt',\n",
              " 'relax',\n",
              " 'requir',\n",
              " 'notion',\n",
              " 'similar',\n",
              " 'degre',\n",
              " 'freedom',\n",
              " 'outsid',\n",
              " 'scope',\n",
              " 'paper',\n",
              " 'goal',\n",
              " 'section',\n",
              " 'provid',\n",
              " 'asymptot',\n",
              " 'bind',\n",
              " 'expect',\n",
              " 'depend',\n",
              " 'smallest',\n",
              " 'zero',\n",
              " 'eigenvalu',\n",
              " 'arbitrarili',\n",
              " 'small',\n",
              " 'scale',\n",
              " 'theorem',\n",
              " 'assum',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'obtain',\n",
              " 'proof',\n",
              " 'techniqu',\n",
              " 'adapt',\n",
              " 'extend',\n",
              " 'proof',\n",
              " 'techniqu',\n",
              " 'base',\n",
              " 'nonasymptot',\n",
              " 'expans',\n",
              " 'power',\n",
              " 'result',\n",
              " 'studi',\n",
              " 'recurs',\n",
              " 'replac',\n",
              " 'expect',\n",
              " 'detail',\n",
              " 'optim',\n",
              " 'bound',\n",
              " 'bind',\n",
              " 'lead',\n",
              " 'rate',\n",
              " 'know',\n",
              " 'optim',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'reason',\n",
              " 'assumpt',\n",
              " 'algorithm',\n",
              " 'complex',\n",
              " 'averag',\n",
              " 'better',\n",
              " 'depend',\n",
              " 'term',\n",
              " 'unimprov',\n",
              " 'initi',\n",
              " 'condit',\n",
              " 'small',\n",
              " 'initi',\n",
              " 'condit',\n",
              " 'forget',\n",
              " 'slowli',\n",
              " 'note',\n",
              " 'addit',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'assumpt',\n",
              " 'initi',\n",
              " 'condit',\n",
              " 'forget',\n",
              " 'faster',\n",
              " 'exponenti',\n",
              " 'fast',\n",
              " 'averag',\n",
              " 'tradit',\n",
              " 'use',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'specif',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'averag',\n",
              " 'iter',\n",
              " 'sequenc',\n",
              " 'homogen',\n",
              " 'markov',\n",
              " 'chain',\n",
              " 'appropri',\n",
              " 'technic',\n",
              " 'condit',\n",
              " 'markov',\n",
              " 'chain',\n",
              " 'uniqu',\n",
              " 'stationari',\n",
              " 'invari',\n",
              " 'distribut',\n",
              " 'sequenc',\n",
              " 'iter',\n",
              " 'converg',\n",
              " 'distribut',\n",
              " 'invari',\n",
              " 'distribut',\n",
              " 'chapter',\n",
              " 'denot',\n",
              " 'invari',\n",
              " 'distribut',\n",
              " 'assum',\n",
              " 'markov',\n",
              " 'chain',\n",
              " 'harri',\n",
              " 'recurr',\n",
              " 'ergod',\n",
              " 'theorem',\n",
              " 'harri',\n",
              " 'markov',\n",
              " 'chain',\n",
              " 'show',\n",
              " 'mean',\n",
              " 'converg',\n",
              " 'sure',\n",
              " 'stationari',\n",
              " 'distribut',\n",
              " 'take',\n",
              " 'expect',\n",
              " 'show',\n",
              " 'limn',\n",
              " 'invert',\n",
              " 'slight',\n",
              " 'stronger',\n",
              " 'assumpt',\n",
              " 'show',\n",
              " 'limn',\n",
              " 'denot',\n",
              " 'covari',\n",
              " 'markov',\n",
              " 'chain',\n",
              " 'start',\n",
              " 'stationar',\n",
              " 'impli',\n",
              " 'limn',\n",
              " 'finit',\n",
              " 'limit',\n",
              " 'interpret',\n",
              " 'explain',\n",
              " 'averag',\n",
              " 'produc',\n",
              " 'sequenc',\n",
              " 'estim',\n",
              " 'converg',\n",
              " 'solut',\n",
              " 'pointwis',\n",
              " 'rate',\n",
              " 'converg',\n",
              " 'order',\n",
              " 'note',\n",
              " 'result',\n",
              " 'stronger',\n",
              " 'independ',\n",
              " 'lowest',\n",
              " 'eigenvalu',\n",
              " 'loss',\n",
              " 'quadrat',\n",
              " 'properti',\n",
              " 'hold',\n",
              " 'mean',\n",
              " 'stationari',\n",
              " 'distribut',\n",
              " 'coincid',\n",
              " 'distanc',\n",
              " 'typic',\n",
              " 'order',\n",
              " 'section',\n",
              " 'converg',\n",
              " 'higher',\n",
              " 'order',\n",
              " 'go',\n",
              " 'consid',\n",
              " 'extra',\n",
              " 'assumpt',\n",
              " 'order',\n",
              " 'bind',\n",
              " 'moment',\n",
              " 'excess',\n",
              " 'risk',\n",
              " 'high',\n",
              " 'probabl',\n",
              " 'bind',\n",
              " 'real',\n",
              " 'number',\n",
              " 'greater',\n",
              " 'exist',\n",
              " 'condit',\n",
              " 'say',\n",
              " 'kurtosi',\n",
              " 'project',\n",
              " 'covari',\n",
              " 'direct',\n",
              " 'bound',\n",
              " 'note',\n",
              " 'comput',\n",
              " 'constant',\n",
              " 'happen',\n",
              " 'equival',\n",
              " 'optim',\n",
              " 'problem',\n",
              " 'solv',\n",
              " 'fastica',\n",
              " 'algorithm',\n",
              " 'provid',\n",
              " 'estim',\n",
              " 'tabl',\n",
              " 'provid',\n",
              " 'estim',\n",
              " 'spars',\n",
              " 'dataset',\n",
              " 'experi',\n",
              " 'consid',\n",
              " 'direct',\n",
              " 'ax',\n",
              " 'high',\n",
              " 'dimension',\n",
              " 'spars',\n",
              " 'dataset',\n",
              " 'dataset',\n",
              " 'give',\n",
              " 'variabl',\n",
              " 'equal',\n",
              " 'zero',\n",
              " 'observ',\n",
              " 'typic',\n",
              " 'larg',\n",
              " 'adapt',\n",
              " 'analyz',\n",
              " 'normal',\n",
              " 'techniqu',\n",
              " 'like',\n",
              " 'improv',\n",
              " 'theoret',\n",
              " 'robust',\n",
              " 'algorithm',\n",
              " 'note',\n",
              " 'result',\n",
              " 'expect',\n",
              " 'theorem',\n",
              " 'theorem',\n",
              " 'provid',\n",
              " 'bind',\n",
              " 'moment',\n",
              " 'excess',\n",
              " 'risk',\n",
              " 'theorem',\n",
              " 'assum',\n",
              " 'real',\n",
              " 'step',\n",
              " 'size',\n",
              " 'note',\n",
              " 'control',\n",
              " 'order',\n",
              " 'moment',\n",
              " 'smaller',\n",
              " 'step',\n",
              " 'size',\n",
              " 'need',\n",
              " 'scale',\n",
              " 'provid',\n",
              " 'high',\n",
              " 'probabl',\n",
              " 'bind',\n",
              " 'tail',\n",
              " 'decay',\n",
              " 'polynomi',\n",
              " 'smaller',\n",
              " 'step',\n",
              " 'size',\n",
              " 'lighter',\n",
              " 'tail',\n",
              " 'corollari',\n",
              " 'step',\n",
              " 'size',\n",
              " 'squar',\n",
              " 'estim',\n",
              " 'section',\n",
              " 'show',\n",
              " 'squar',\n",
              " 'regress',\n",
              " 'averag',\n",
              " 'achiev',\n",
              " 'converg',\n",
              " 'rate',\n",
              " 'assumpt',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'loss',\n",
              " 'constant',\n",
              " 'stepsiz',\n",
              " 'stationari',\n",
              " 'distribut',\n",
              " 'correspond',\n",
              " 'homogen',\n",
              " 'markov',\n",
              " 'chain',\n",
              " 'satisfi',\n",
              " 'rthe',\n",
              " 'general',\n",
              " 'error',\n",
              " 'gradient',\n",
              " 'linear',\n",
              " 'quadrat',\n",
              " 'impli',\n",
              " 'averag',\n",
              " 'recurs',\n",
              " 'converg',\n",
              " 'pathwis',\n",
              " 'coincid',\n",
              " 'withr',\n",
              " 'optim',\n",
              " 'valu',\n",
              " 'defin',\n",
              " 'gradient',\n",
              " 'longer',\n",
              " 'linear',\n",
              " 'general',\n",
              " 'estim',\n",
              " 'problem',\n",
              " 'expect',\n",
              " 'averag',\n",
              " 'sequenc',\n",
              " 'converg',\n",
              " 'rate',\n",
              " 'mean',\n",
              " 'stationari',\n",
              " 'distribut',\n",
              " 'optim',\n",
              " 'predictor',\n",
              " 'typic',\n",
              " 'averag',\n",
              " 'distanc',\n",
              " 'order',\n",
              " 'section',\n",
              " 'averag',\n",
              " 'iter',\n",
              " 'converg',\n",
              " 'pointwis',\n",
              " 'order',\n",
              " 'strong',\n",
              " 'convex',\n",
              " 'problem',\n",
              " 'addit',\n",
              " 'smooth',\n",
              " 'condit',\n",
              " 'loss',\n",
              " 'function',\n",
              " 'satisfi',\n",
              " 'exampl',\n",
              " 'logist',\n",
              " 'loss',\n",
              " 'quadrat',\n",
              " 'function',\n",
              " 'optim',\n",
              " 'rate',\n",
              " 'weak',\n",
              " 'condit',\n",
              " 'go',\n",
              " 'quadrat',\n",
              " 'approxim',\n",
              " 'choos',\n",
              " 'support',\n",
              " 'point',\n",
              " 'share',\n",
              " 'similar',\n",
              " 'newton',\n",
              " 'procedur',\n",
              " 'trivial',\n",
              " 'adapt',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'framework',\n",
              " 'newton',\n",
              " 'step',\n",
              " 'certain',\n",
              " 'point',\n",
              " 'equival',\n",
              " 'minim',\n",
              " 'quadrat',\n",
              " 'surrog',\n",
              " 'newton',\n",
              " 'step',\n",
              " 'solv',\n",
              " 'approxim',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'follow',\n",
              " 'recurs',\n",
              " 'equival',\n",
              " 'replac',\n",
              " 'gradient',\n",
              " 'order',\n",
              " 'approxim',\n",
              " 'crucial',\n",
              " 'point',\n",
              " 'machin',\n",
              " 'learn',\n",
              " 'scenario',\n",
              " 'loss',\n",
              " 'associ',\n",
              " 'singl',\n",
              " 'data',\n",
              " 'point',\n",
              " 'complex',\n",
              " 'twice',\n",
              " 'complex',\n",
              " 'regular',\n",
              " 'stochast',\n",
              " 'approxim',\n",
              " 'step',\n",
              " 'rank',\n",
              " 'matrix',\n",
              " 'choic',\n",
              " 'support',\n",
              " 'point',\n",
              " 'quadrat',\n",
              " 'approxim',\n",
              " 'import',\n",
              " 'aspect',\n",
              " 'choic',\n",
              " 'paper',\n",
              " 'consid',\n",
              " 'strategi',\n",
              " 'support',\n",
              " 'point',\n",
              " 'step',\n",
              " 'procedur',\n",
              " 'convex',\n",
              " 'loss',\n",
              " 'averag',\n",
              " 'step',\n",
              " 'size',\n",
              " 'decay',\n",
              " 'achiev',\n",
              " 'rate',\n",
              " 'logarithm',\n",
              " 'term',\n",
              " 'obtain',\n",
              " 'decent',\n",
              " 'estim',\n",
              " 'stage',\n",
              " 'procedur',\n",
              " 'follow',\n",
              " 'use',\n",
              " 'observ',\n",
              " 'step',\n",
              " 'averag',\n",
              " 'averag',\n",
              " 'constant',\n",
              " 'step',\n",
              " 'size',\n",
              " 'obtain',\n",
              " 'newton',\n",
              " 'step',\n",
              " 'show',\n",
              " 'algorithm',\n",
              " 'achiev',\n",
              " 'rate',\n",
              " 'logist',\n",
              " 'regress',\n",
              " 'effici',\n",
              " 'practic',\n",
              " 'support',\n",
              " 'point',\n",
              " 'current',\n",
              " 'averag',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "processed_text[4310]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BwPhuPQ-DTH",
        "outputId": "6bc624ed-e79c-4cdb-e400-cb26eadde28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Score: 0.7299064993858337\t \n",
            "Topic: 0.011*\"bind\" + 0.011*\"loss\" + 0.011*\"regret\" + 0.010*\"theorem\" + 0.010*\"bound\" + 0.009*\"convex\" + 0.008*\"gradient\" + 0.006*\"onlin\" + 0.006*\"proof\" + 0.006*\"lemma\"\n",
            "\n",
            "Score: 0.23240989446640015\t \n",
            "Topic: 0.025*\"kernel\" + 0.010*\"theorem\" + 0.009*\"loss\" + 0.008*\"norm\" + 0.007*\"regular\" + 0.007*\"regress\" + 0.007*\"rank\" + 0.007*\"bound\" + 0.006*\"label\" + 0.006*\"risk\"\n",
            "\n",
            "Score: 0.03699308633804321\t \n",
            "Topic: 0.009*\"gradient\" + 0.009*\"infer\" + 0.008*\"variat\" + 0.007*\"layer\" + 0.006*\"word\" + 0.006*\"cluster\" + 0.006*\"stochast\" + 0.006*\"latent\" + 0.006*\"dataset\" + 0.006*\"likelihood\"\n"
          ]
        }
      ],
      "source": [
        "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcNfoQXvAQj-"
      },
      "source": [
        "**Testing Model on unseen data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG05Tihk-SHc",
        "outputId": "a7091d9a-9ce7-4ded-93e9-456dd13fe75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.699935257434845\t Topic: 0.018*\"neuron\" + 0.016*\"activ\" + 0.013*\"spike\" + 0.010*\"layer\" + 0.008*\"unit\"\n",
            "Score: 0.03334445133805275\t Topic: 0.035*\"imag\" + 0.008*\"dataset\" + 0.008*\"layer\" + 0.007*\"nois\" + 0.006*\"convolut\"\n",
            "Score: 0.03334223851561546\t Topic: 0.014*\"neuron\" + 0.012*\"unit\" + 0.011*\"cell\" + 0.009*\"activ\" + 0.008*\"respons\"\n",
            "Score: 0.033342111855745316\t Topic: 0.013*\"rank\" + 0.010*\"user\" + 0.010*\"graph\" + 0.007*\"item\" + 0.005*\"game\"\n",
            "Score: 0.03334101289510727\t Topic: 0.011*\"bind\" + 0.011*\"loss\" + 0.011*\"regret\" + 0.010*\"theorem\" + 0.010*\"bound\"\n",
            "Score: 0.0333402119576931\t Topic: 0.009*\"gradient\" + 0.009*\"infer\" + 0.008*\"variat\" + 0.007*\"layer\" + 0.006*\"word\"\n",
            "Score: 0.03333904594182968\t Topic: 0.008*\"imag\" + 0.007*\"action\" + 0.007*\"layer\" + 0.007*\"agent\" + 0.006*\"sequenc\"\n",
            "Score: 0.033338841050863266\t Topic: 0.016*\"tree\" + 0.009*\"sequenc\" + 0.008*\"node\" + 0.007*\"graph\" + 0.006*\"nod\"\n",
            "Score: 0.033338673412799835\t Topic: 0.020*\"polici\" + 0.017*\"cluster\" + 0.010*\"action\" + 0.008*\"graph\" + 0.007*\"reward\"\n",
            "Score: 0.03333815187215805\t Topic: 0.025*\"kernel\" + 0.010*\"theorem\" + 0.009*\"loss\" + 0.008*\"norm\" + 0.007*\"regular\"\n"
          ]
        }
      ],
      "source": [
        "unseen_text = 'I like cookies with extra chocolate chips'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_text))\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "J051 NLP Assignment 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}